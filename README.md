# AI-in-the-Real-World-judge the bot  
## Description
Assignment Brief (student-facing)

You’re now a Responsible AI Inspector . Your job is to investigate how AI is used in a scenario, spot anything suspicious (bias? lack of transparency?), and help fix it. You'll be given 2 short cases.

Your mission:

Describe what the AI is doing.

Spot what could go wrong (hint: check fairness, privacy, accountability...)

Suggest 1 way to improve it responsibly.

Bonus points if you write your answer like a short blog post explaining the issue in a fun and clear way.

Think of it as being a detective — but make it vibe. 

Example Scenario Prompts

Hiring Bot: A company uses an AI to screen job applicants. It tends to reject more female applicants with career gaps.

School Proctoring AI: A system flags students as "cheating" based on eye movement — but it often flags neurodivergent students.

## answer
Case File #021: The Biased Hiring Bot
Location: HR Department, TechCorp
Suspect: Resume Screening AI
Status: Under investigation

What’s Going On?

Meet HireMate3000 — a fancy AI tool TechCorp uses to sift through thousands of job applications. It’s supposed to make hiring faster by flagging the best candidates based on experience, skills, and past performance data.

But here's the twist: our AI friend has a pattern — it keeps rejecting women who took career breaks, especially those related to parenting. Coincidence? Doubtful.

What’s the Problem?
Red flag alert: The AI was trained on historical hiring data — which already had biases baked in. If past decisions favored continuous male work histories, the AI simply learned that “career gap = bad.” So it repeats that bias, making it harder for qualified women to even get an interview.

This isn’t just unfair — it could violate equal opportunity laws, and it certainly hurts diversity and inclusion efforts.

Also: no one really knows how the AI is making decisions. No transparency = no accountability.

How to Fix It?

One fix? Audit the AI’s training data. Filter out features like gender and career gaps if they’re leading to biased outcomes. Even better: retrain the model on fair, balanced datasets that reflect today’s inclusive hiring values.

And hey, why not throw in a “bias buster” review layer — a human check to ensure the AI’s not making sketchy calls before rejecting someone’s dream job.

Bonus Vibes: What We Learned
AI isn’t evil — but it learns from us. If we feed it biased data, it becomes a digital echo chamber for injustice. As AI Inspectors, our job is to question, correct, and create tech that’s not just smart — but responsible.

Case File #022: The Suspicious School Surveillance
Case File #022: The Shifty-Eyed Student Scandal
 Location: Online Exam Hall (a.k.a. your bedroom)
 Suspect: ProctorAI™ – Automated Exam Monitoring System
 Status: Trigger-happy

What’s Going On?
Picture this: you're taking an online exam, focused, calm, maybe even chewing your pen — when suddenly, your screen freezes.
You’ve been flagged for suspicious behavior.

What did you do?
Apparently, your eyes moved too much — or you looked away from the screen.
Welcome to the world of ProctorAI, the digital hall monitor powered by facial recognition and eye-tracking. It watches for "unusual" behaviors to catch cheating… but its definition of "unusual"? Super flawed.

What’s the Problem?
Here's the twist: it’s flagging neurodivergent students — like those with ADHD, autism, or anxiety — more often than others.

Why? Because:
They might fidget, look around, or stim.
They may need to process questions in non-linear ways.
Their behavior doesn’t match the AI’s narrow idea of “normal test-taking posture.”
That’s not just unfair. It’s ableist.
And worst of all? There’s little transparency in how the AI decides what’s “cheating.” Students are left stressed, flagged, and in some cases — penalized — with no real appeal process.

How to Fix It?
One key fix: Redesign ProctorAI with neurodiversity in mind. That means:
Training the model on inclusive behavioral data (not just on neurotypical test-takers).
Building in explainability — students should know why they were flagged.
And adding a human-in-the-loop review system, so AI doesn’t get the final say on academic integrity.
Also, schools should offer alternatives for students with different needs — because one-size-fits-all rarely fits anyone well.
Bonus Vibes: What We Learned
Just because an AI watches doesn't mean it understands.
Being a Responsible AI Inspector means asking: Who does this help? Who does it hurt? And how do we build systems that treat all students fairly — eyes wandering or not?
AI should support learning, not punish diversity.
Stay curious. Stay critical.
