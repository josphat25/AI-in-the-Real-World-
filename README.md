# AI-in-the-Real-World-judge the bot  
## Description
Assignment Brief (student-facing)

You’re now a Responsible AI Inspector . Your job is to investigate how AI is used in a scenario, spot anything suspicious (bias? lack of transparency?), and help fix it. You'll be given 2 short cases.

Your mission:

Describe what the AI is doing.

Spot what could go wrong (hint: check fairness, privacy, accountability...)

Suggest 1 way to improve it responsibly.

Bonus points if you write your answer like a short blog post explaining the issue in a fun and clear way.

Think of it as being a detective — but make it vibe. 

Example Scenario Prompts

Hiring Bot: A company uses an AI to screen job applicants. It tends to reject more female applicants with career gaps.

School Proctoring AI: A system flags students as "cheating" based on eye movement — but it often flags neurodivergent students.

## answer
Case File #021: The Biased Hiring Bot
Location: HR Department, TechCorp
Suspect: Resume Screening AI
Status: Under investigation

What’s Going On?

Meet HireMate3000 — a fancy AI tool TechCorp uses to sift through thousands of job applications. It’s supposed to make hiring faster by flagging the best candidates based on experience, skills, and past performance data.

But here's the twist: our AI friend has a pattern — it keeps rejecting women who took career breaks, especially those related to parenting. Coincidence? Doubtful.

What’s the Problem?
Red flag alert: The AI was trained on historical hiring data — which already had biases baked in. If past decisions favored continuous male work histories, the AI simply learned that “career gap = bad.” So it repeats that bias, making it harder for qualified women to even get an interview.

This isn’t just unfair — it could violate equal opportunity laws, and it certainly hurts diversity and inclusion efforts.

Also: no one really knows how the AI is making decisions. No transparency = no accountability.

How to Fix It?

One fix? Audit the AI’s training data. Filter out features like gender and career gaps if they’re leading to biased outcomes. Even better: retrain the model on fair, balanced datasets that reflect today’s inclusive hiring values.

And hey, why not throw in a “bias buster” review layer — a human check to ensure the AI’s not making sketchy calls before rejecting someone’s dream job.

Bonus Vibes: What We Learned
AI isn’t evil — but it learns from us. If we feed it biased data, it becomes a digital echo chamber for injustice. As AI Inspectors, our job is to question, correct, and create tech that’s not just smart — but responsible.

Case File #022: The Suspicious School Surveillance
Case File #022: The Shifty-Eyed Student Scandal
 Location: Online Exam Hall (a.k.a. your bedroom)
 Suspect: ProctorAI™ – Automated Exam Monitoring System
 Status: Trigger-happy

What’s Going On?
Picture this: you're taking an online exam, focused, calm, maybe even chewing your pen — when suddenly, your screen freezes.
You’ve been flagged for suspicious behavior.

What did you do?
Apparently, your eyes moved too much — or you looked away from the screen.
Welcome to the world of ProctorAI, the digital hall monitor powered by facial recognition and eye-tracking. It watches for "unusual" behaviors to catch cheating… but its definition of "unusual"? Super flawed.

What’s the Problem?
Here's the twist: it’s flagging neurodivergent students — like those with ADHD, autism, or anxiety — more often than others.

Why? Because:
They might fidget, look around, or stim.
They may need to process questions in non-linear ways.
Their behavior doesn’t match the AI’s narrow idea of “normal test-taking posture.”
That’s not just unfair. It’s ableist.
And worst of all? There’s little transparency in how the AI decides what’s “cheating.” Students are left stressed, flagged, and in some cases — penalized — with no real appeal process.

How to Fix It?
One key fix: Redesign ProctorAI with neurodiversity in mind. That means:
Training the model on inclusive behavioral data (not just on neurotypical test-takers).
Building in explainability — students should know why they were flagged.
And adding a human-in-the-loop review system, so AI doesn’t get the final say on academic integrity.
Also, schools should offer alternatives for students with different needs — because one-size-fits-all rarely fits anyone well.
Bonus Vibes: What We Learned
Just because an AI watches doesn't mean it understands.
Being a Responsible AI Inspector means asking: Who does this help? Who does it hurt? And how do we build systems that treat all students fairly — eyes wandering or not?

## Responsible AI Inspector Report
By: Josphat Ndungu
Case 1: The Biased Hiring Bot
 What’s Happening:
A company uses an AI tool to screen job applicants. It analyzes resumes and filters out candidates based on patterns it has learned from historical hiring data.

 What’s Problematic:
The AI tends to reject women with career gaps — often due to maternity or caregiving. It has learned gender bias from past data and is repeating it, unfairly penalizing qualified applicants. Plus, no one really knows how it makes its decisions — it’s a black box.

 One Improvement Idea:
Audit and retrain the AI on fairer, balanced data that doesn’t discriminate based on career gaps or gender. Add a human review layer to check for biased decisions before rejections are final.

 Bonus Blog Vibe:

"HireMate3000 might be efficient, but it’s also a little sexist. When algorithms inherit our flaws, they automate injustice. Time to teach our robots some respect — and fairness!"

Case 2: The Suspicious School Surveillance
 What’s Happening:
A school uses an AI-powered proctoring system during online exams. It monitors students’ eye movement, head turns, and facial expressions to detect possible cheating.

 What’s Problematic:
The system often flags neurodivergent students who naturally fidget, look around, or struggle with eye contact. This leads to false accusations of cheating. There's little transparency and no fair appeal process — just an automated accusation.

 One Improvement Idea:
Make the AI neurodiversity-aware by training it on a more inclusive dataset. Also, involve human reviewers before flagging behavior as cheating, and offer accommodations for students with special needs.

 Bonus Blog Vibe:

"Just because a student looks around doesn’t mean they’re cheating — maybe their brain just works differently. AI should help us learn, not label us unfairly. Let’s build smarter tools that know the difference!"

Done:

Two cases analyzed

Issues identified (bias, fairness, transparency)

One solution per case

Bonus: Delivered in fun, readable blog format
AI should support learning, not punish diversity.
Stay curious. Stay critical.
